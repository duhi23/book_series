\chapter*{ANEXO A}
%\label{sec:anexo}
\section*{A.1: NOCIONES SOBRE PROCESOS ESTOC\'{A}STICOS}
%\label{subsec:anexo}

Sea el proceso estoc\'{a}stico $\left( {X_{t} } \right)_{t\in T}$, definido sobre el espacio de probabilidad $(\Omega ,A ,P)$ y a valores en $(E,\beta )$. $(\Omega ,A ,P)$ se dice espacio de base y $(E,\beta )$ espacio de estados. Se definen adem\'{a}s:

\begin{itemize}
\item $t\to X_{t} \left( w \right):$ trayectoria de $w$ $\left( {w\in \Omega } \right)$
\item $w\to X_{t} \left( w \right):$ estado del proceso en el tiempo (o instante) t ($t\in T)$
\item $T$: Conjunto de tiempos
\end{itemize}

Un proceso estoc\'{a}stico es una aplicaci\'{o}n $X$:
\[
X: (\Omega ,A ,P) \quad \to\quad (E^{T}, S ,P_{X} )
\]
\[
w\to \left( {X_{t} \left( w \right),t\in T} \right)
\]

$P_{X}$ es la ley del proceso, $\pi_{t_{0}}$ es la proyecci\'{o}n tal que $\pi_{t_{0} } \left( {X_{t} , t\in T} \right)=X_{t_{0} }$ y $S=\sigma \left( {\pi_{t}, t\in T} \right)$ la $\sigma$- \'{a}lgebra generada por las $\left( {\pi_{t} } \right)_{t\in T} $.

\begin{itemize}
      \item X es medible pues: 
\[
X_{t} =\pi_{t} \circ X\Rightarrow \forall \quad B\in \beta,\quad X_{t}^{-1} \left( B \right)=X^{-1}\left( {\pi_{t}^{-1} \left( B \right)} \right)
\]
Pero los conjuntos $\pi_{t}^{-1} (B)$ generan S y $X_{t}^{-1} (B)\in A$, por lo cual $X^{-1} (\pi_{t}^{-1} (B))\in A$.
      
      \item $\left( {\pi_{t}, t\in T} \right)$ se dice el proceso can\'{o}nico asociado a $\left( {X_{t}, t\in T} \right)$.
      \item Leyes de dimensi\'{o}n finita son aquellas de $\left( {X_{t_{1} } ,\ldots, X_{t_{k} } } \right), k=1,2, \ldots;t_{1},\ldots,t_{k} \in T$
\end{itemize}

A continuaci\'{o}n se presentan dos resultados importantes, cuyas demostraciones se admiten.\newline

\textbf{Teorema de Kolmogorov:} Sean $E=R^{d}$, $\beta =\beta_{R^{d}} $. Entonces, la ley del proceso est\'{a} determinada por las leyes de dimensi\'{o}n finita (bajo ciertas condiciones llamadas de proyectividad).\newline

\textbf{Observaci\'{o}n A1:} Cuando estas leyes de dimensi\'{o}n finita son gaussianas, la ley del proceso X est\'{a} determinada por:
\begin{itemize}
      \item media: $t\to EX(t)$.
      \item covarianza: $\left( {s,t} \right)\to cov\left( {X_{s}, X_{t} } \right)$.
\end{itemize}

\section*{A.2: DEMOSTRACIONES DE ALGUNOS TEOREMAS DEL CAP\'{I}TULO 1}
%\label{subsec:mylabel2}
\textbf{Teorema 1.1:} Existe una medida \'{u}nica $\mu$ acotada y sim\'{e}trica sobre $\left[ {-\pi ,\pi } \right]$ tal que:
\[
\gamma_{t} =\int\limits_{\left[ {-\pi ,\pi } \right]} {\cos \lambda t d\mu \left( \lambda \right)}, \quad t\in Z
\]
$\mu$ se dice la medida espectral de $\left( {X_{t} } \right)$.\newline

\textbf{Demostraci\'{o}n:}\newline

\begin{itemize}
      \item \textbf{Caso particular:} $\sum\limits_t {\left| {\gamma_{t} } \right|<\infty }$\newline

Se puede definir $f\left( \lambda \right)=\frac{1}{2\pi }\sum\limits_{t\in Z} {\gamma_{t} \cos \lambda t}$, $\lambda \in \left[ {-\pi ,\pi } \right]$ (f est\'{a} bien definida).\newline

Entonces: 
\[
\gamma_{t} =\int_{-\Pi }^\Pi {\cos \lambda t} 
f\left( \lambda \right)d\lambda 
\]
Si se llega a demostrar que $f\ge 0$, entonces $f\left( \lambda \right){\kern 1pt}{\kern 1pt}d\lambda $ puede considerarse como $d\mu \left( \lambda \right)$.

      \item En caso contrario, consid\'{e}rese: 
\[
I_{T} (\lambda)=\frac{1}{2\pi T}\sum\limits_{1\le s,t\le T} {X_{s} } X_{t} e^{i\lambda (t-s)},\quad \lambda \in \left[-\pi, \pi \right], \quad T=1,2,\ldots
\]
$I_{T}(\lambda)$ se llama el periodograma.\newline

Se puede observar que:
\begin{align*}
I_{T} \left( \lambda \right) & = \frac{1}{2\pi T}\left| {\sum\limits_{t=1}^T {X_{t} e^{i\lambda t}} } \right|^{2}\ge 0\\
                             & = \frac{1}{2\pi T}\sum\limits_{1\le s,t\le T} {X_{s} X_{t} \cos ( t-s)} \quad \text{pues}\quad \sum_{1\le s,t\le T}^{X_{s} X_{t} } sen\lambda (t-s)=0
\end{align*}

Se considera ahora: 

\begin{align*}
f_{T} (\lambda) &= EI_{T} (\lambda)=\frac{1}{2\pi T}\sum\limits_{1\leq s,t\leq T} {E\left[ {\left(X_{t} X_{s} \right)\cos \lambda (t-s)} \right]\ge 0}\\
                &= \frac{1}{2\pi T}\sum\limits_{s,t} {\gamma_{t-s} } \cos \lambda \left( {t-s} \right)\\
                &= \frac{1}{2\pi T}\sum\limits_{h=-(T-1)}^{T-1} {\left(T-|h| \right)}\gamma_{h} \cos \lambda h\quad \text{(poniendo $h=t-s$)}\\
                &= \frac{1}{2\pi }\sum\limits_{t=-(T-1)}^{T-1} {\left( {1-\frac{\left| t \right|}{T}} \right)} \gamma_{t} \cos \lambda t
\end{align*}

Sea $F_{T}(\lambda)=\int_{-\pi }^\lambda {f_{T} (v)dv=\frac{\gamma_{o}}{2\pi}(\lambda +\pi)+\frac{1}{\pi}\displaystyle\sum\limits_{t=1}^{T-1} {\left( {1-\frac{t}{T}} \right)}\gamma_{t} \frac{sen\lambda t}{t}}$ entonces: $F_{T}$ es creciente (pues $f_{T} \ge 0$),  $F_{T}(-\pi)=0$ y $F_{T}(\pi)=\gamma_{0}$.

\end{itemize}

Se recuerda el lema de Helly -- Bray:\newline

Dada una sucesi\'{o}n $\left( {F_{T} ,T\in N^{\ast }} \right)$ con la misma masa total, se puede extraer una subsucesi\'{o}n $\left( F_{T^{'}} \right)$, que converge estrictamente (en ley).\newline

Es decir, existe una funci\'{o}n de distribuci\'{o}n F, con una medida $\mu $, de masa total $\gamma_{o}$, t.q. $F_{T^{'}} \to F$ en todo punto de continuidad de F.\newline

As\'{i}, si $\mu_{T}$ es la medida asociada a $F_{T}$, entonces $\forall \phi$ continua (acotada ) sobre $\left[ {-\pi ,\pi } \right]$, 

\[\int \phi d\mu_{T^{'}} \to \int \phi d\mu \]

As\'{i}, tomando $\varphi \left( \lambda \right)=\cos \lambda t$, se tiene que:
\[
\int_{\left[-\pi,\pi \right]} \cos (\lambda t) d\mu_{T^{'}} (\lambda)= \int_{\left[-\pi, \pi\right]} \cos (\lambda t) f_{T^{'}} (\lambda ) d\lambda 
\]
pero 
\[\int\limits_{\left[ {-\pi ,\pi } \right]} \cos (\lambda t) d\mu_{T^{'}} (\lambda)\to \int\limits_{\left[-\pi, \pi \right]} \cos (\lambda t) d\mu (\lambda)\quad \text{cuando}\quad T^{'}\to \infty \]
y
\[\int\limits_{\left[-\pi, \pi \right]} \cos (\lambda t) f_{T^{'}} (\lambda)=\left( 1-\frac{|t|}{T'} \right)\gamma_{t} \to \gamma_{t}\quad \text{cuando}\quad T^{'}\to \infty \]

\[
\int_{\left[-\pi, \pi \right]} \cos (\lambda t) d\mu (\lambda)=\gamma_{t} 
\]

Sean: 
\[
G_{T}(\lambda)=F_{T} (\lambda )-\frac{\gamma_{O}}{2\pi}(\lambda +\pi)=\frac{1}{\pi}\sum\limits_{t=1}^{T-1} 
\left(1-\frac{t}{T}\right) \gamma_{t}\frac{sen\lambda t}{t}
\] 
y
\[
G(\lambda)=F(\lambda)-\frac{\gamma_{O}}{2\pi}(\lambda +\pi)
\]
Ahora
\[
\int_{-\pi}^{T} \sen (\lambda t) G_{T^{'}} (\lambda) d\lambda \to \int_{-\pi}^{\pi} \sen (\lambda t) G(\lambda) d\lambda
\]
(por el teorema de convergencia dominada (TCD), pues $G_{T^{'}}$ est\'{a} mayorada por una constante y $G_{T^{'}} \to G$, en todo punto de continuidad de G).\newline

AdemÃ¡s:
\[
\left(1-\frac{t}{T^{'}} \right)\frac{\gamma_{t}}{t}\to \frac{\gamma_{t}}{t}\quad \text{cuando}\quad T^{'}\to \infty
\]
Por tanto:
\[
F(\lambda)=\frac{\gamma_{O}}{2\pi}(\lambda +\pi) + \frac{1}{\pi}\sum_{t=1}^{\infty} \gamma_{t} \frac{\sen \lambda t}{t}
\]
Puesto que F es creciente y de variaci\'{o}n acotada, entonces la serie converge en todo punto de continuidad de F.\newline

Una medida $\mu$ asociada a F se dice, medida central del proceso.

\begin{itemize}
      \item \textbf{?`Unicidad de } $\mu$ \textbf{?}
\end{itemize}

En general $\mu$ no es \'{u}nica. Por ejemplo:\newline

Sea $X_{t}=(-1)^{t}X$ con $EX=0$ y $EX^{2}=1$. Se ha visto que $\left(X_{t}\right)$ es d.e. y $\gamma_{t}=\left(-1 \right)^{t}$,  $t\in Z$.\newline

Sea $\mu_{\alpha} =\alpha \delta_{(-\pi)} + (1-\alpha) \delta_{(\pi)}$, con $\alpha \in \left[0, 1\right]$, donde $\delta_{(a)}$ denota a la medida de Dirac en a.\newline

Dado que $\int_{-\pi}^{\pi} sen\lambda t d\mu_{\alpha} (\lambda)=0$ se tiene que:
\[
\int_{-\pi}^{\pi} \cos \lambda t d\mu_{\alpha} (\lambda)=\cos \pi t=\left(-1 \right)^{t}\quad\forall \alpha \in \left[0,1 \right],\quad \forall t\in Z
\]

Por tanto, $\gamma_{t}$ se expresa de la forma requerida, pero hay un n\'{u}mero infinito de medidas centrales; sin embargo, si se exige que la medida sea sim\'{e}trica, se tiene la unicidad. En el ejemplo precedente: 
\[
\mu =\frac{1}{2}\left[ {\delta_{\left( {-\pi } \right)} +\delta_{\left( 
\pi \right)} } \right]
\]

La funci\'{o}n de repartici\'{o}n $F$ que se ha obtenido es la funci\'{o}n de repartici\'{o}n de una medida sim\'{e}trica sobre $\left] {-\pi,\pi } \right[$. Sea $\mu$ la medida central y se define:

\[
\nu \left( {\text{A}} \right)=\mu \left( A \right)\quad \forall A\in \beta (\left] {-\pi ,\pi } \right])
\]

\[
\nu \left( {-\pi } \right)=\nu \left( \pi \right)=\frac{1}{2}\left[ {\mu \left( {-\pi } \right)+\mu \left( \pi \right)} \right] \quad \left( {\int_{-\pi }^\pi {\cos \lambda t} d\nu \left( \lambda \right)=\int_{-\pi }^\pi {\cos \lambda td\mu \left( \lambda \right)} } \right)
\]

Entonces, se puede considerar $\mu $ sim\'{e}trica

\begin{itemize}
\item \textbf{Unicidad de la medida espectral sim\'{e}trica sobre }$\left]-\pi, \pi \right[$
\end{itemize}

$\int {\cos \lambda t d\nu (t)=\gamma_{t} }$ y $\int {sen\lambda td\nu \left( t \right)=0} $ (conocidos los coeficientes de Fourier).\newline

entonces $\nu$ es conocida sobre los polinomios trigonom\'{e}tricos y por el teorema de Stone-Weierstrass, $\nu $ est\'{a} determinada para las funciones continuas.\newline

\textbf{Teorema 1.6: }Condici\'{o}n suficiente de convergencia c.s.\newline

Sea $\left(X_{t}, t\in Z \right)$ de segundo orden, centrado t.q.

\[
V\left(X_{t} +\ldots+X_{t+p-1} \right)\leq kp^{\gamma}\quad \text{k=cte;}\quad 0\leq \gamma <2;\quad p=1,2,\ldots; \quad \forall t\in Z
\]

Entonces: $\bar{{X}}\to 0$ en media cuadr\'{a}tica y casi seguramente 

\textbf{Demostraci\'{o}n:}\newline

Sea $Sn=\sum\limits_{i=1}^n X_{i} \Rightarrow E\left(\frac{Sn}{n} \right)^{2}\leq \frac{kn^{\gamma}}{n^{2}}=\frac{k}{n^{2-\gamma }}\to 0$, existe convergencia en m.c.

\begin{itemize}
\item Convergencia casi segura:
\end{itemize}

Sea $k>\frac{1}{2-\gamma}$, $k\in Z$ entonces: 
\[
P\left[\left|\frac{S_{m^k}}{m^k} \right|\geq \varepsilon \right] \leq \frac{1}{\varepsilon^2} E\left(\frac{S_{m^k} }{m^k} \right)^{2} \leq \frac{1}{\varepsilon^{2} m^{2k}}\bar{k}m^{k\gamma } = \frac{\bar{k}}{\varepsilon^{2}}\frac{1}{m^{k(2-\gamma)}}
\]

\[
\sum P\left(\left|\frac{S_{m}^k}{m^k} \right|\geq \varepsilon \right) <\infty 
\]
La primera desigualdad se justifica por la desigualdad de Chebyshev.\newline

Por el Teorema de Borel-Cantelli $\frac{S_{m^k} }{m^k}\buildrel {c.s.} \over \longrightarrow 0$.\newline 

Ahora, sea $Y_{m} =\max_{m^k < n\leq (m+1)^k} \left|\frac{S_{n} -S_{m^k}}{n} \right|^2$
\[
P\left(Y_{m} \geq \varepsilon \right)\leq \frac{EY_{m} }{\varepsilon}\quad \left( {Y_{m} \geq 0} \right)
\]
pero: 

\begin{align*}
E(Y_{m}) &\leq \frac{E\left(\left|X_{m^{k}+1}\right|+\ldots+\left|X_{(m-1)^k} \right| \right)^2}{m^{2k}}\\
         &= \frac{1}{m^{2k}}\sum_{j,j^{'}} E\left|X_{j} X_{j^{'}} \right|\\
         &\leq \frac{1}{m^{2k}}\sum_{j,j^{'}} \sqrt{EX_{j}^{2}} \sqrt{EX_{j^{'}}^{2}} = \frac{1}{m^{2k}}\left(\sum_j \sqrt{EX_{j}^{2}} \right)^{2}\\
         &\leq \frac{K}{m^{2k}}\left((m+1)^{k}-m^{k} \right)^{2}=K\left[\left(1+\frac{1}{m} \right)^{k}-1\right]^{2}\\
         &\leq K\left(e^{\frac{k}{m}}-1 \right)^{2}\quad \text{pues}\quad \left(1+\frac{1}{m}\right)^{k}\leq e^{\frac{k}{m}}\\
         &\leq K\left(\frac{2k}{m}\right)^{2}\quad \text{(m suficientemente grande)}
\end{align*}

As\'{i} $E\left(Y_{m}\right)\leq \frac{cte}{m^2}$ y por tanto $\sum_m P(Y_{m}\geq \varepsilon) <\infty$ entonces, por el teorema de Borel-Cantelli: $Y_{m} \mathop \to\limits_{m\to \infty }^{c.s.} 0$.\newline

Sea $m=m(n)$ t.q. $\left(m(n) \right)^{k}<n<\left(m(n)+1 \right)^{k}$; $m(n)$ es \'{u}nico y $m(n)\to \infty$ cuando $n\to \infty$; entonces: 
\[
Y_{m\left( n \right)} \buildrel {c.s.} \over \longrightarrow 0
\]
\[
\frac{S_{n} }{n}-\frac{S_{m(n)^{k}}}{n}\buildrel {c.s.} \over \longrightarrow 0
\]

pues: $\frac{S_{m(n)^{k}}}{n}=\frac{S_{m(n)^{k}}}{m(n)^{k}}\ast \frac{m(n)^{k}}{n}\Rightarrow \frac{S_{n}}{n}\buildrel {c.s.} \over \longrightarrow 0$ (dado que $\frac{S_{(m(n))^{k}}}{(m(n))^{k}}\buildrel {c.s.} \over \longrightarrow 0)$.\newline

\textbf{Teorema 1.8:} Sea $(X_{t}, t\in Z)$ d.e, centrado y regular $\left(\sigma^{2}=E\left(X_{t} -\hat{X}_{t} \right)^{2}>0 \right)$.\newline

(Ejercicio: Verificar que $E\left(X_{t} -\hat{X}_{t} \right)^{2}$ no depende de t).\newline

Entonces: $X_{t} =\sum\limits_{j=0}^\infty \lambda_{j} u_{t-j} +v_{t} t\in Z$ donde $(u_{t})$ r.b. (d\'{e}bil) de varianza $\sigma^{2}$, $\lambda_{o} =1$, $\sum\lambda_{j}^{2} <\infty$, $u_{t} \in \mu_{t}$, $u_{t} \bot \mu_{t-1}$, $\left(v_{t} \right)$ centrado, los $u_{t}$ son ortogonales a los $v_{s}$, $v_{t} \in \mathop \cap\limits_{s=0}^{\infty } \mu_{t-s}$ y la descomposici\'{o}n es \'{u}nica.\newline

\textbf{Observaci\'{o}n B.1:} $u_{t} =X_{t} -\hat{X}_{t} \Rightarrow X_{t} =\hat{X}_{t} +u_{t}$. $u_{t}$ se dice la innovaci\'{o}n del proceso y $v_{t}$ la parte determinista del proceso.\newline

\textbf{Demostraci\'{o}n: }\newline

Sea $u_{t} =X_{t} -\hat{X}_{t}$

\begin{itemize}
\item $\{u_{t} \}$ es centrado, pues $\hat{X}_{t}$ es centrada, por ser proyecci\'{o}n ortogonal sobre un s.e.v. generado por v.a. centradas.
\item $Eu_{t}^{2} =\sigma^{2}$
\item $\hat{X}_{t} \in \mu_{t-1} \subset \mu_{t} \Rightarrow u_{t} \in \mu_{t}$
\item Sea $s<t \quad \left. {\begin{array}{l}
 u_{t} =X_{t} -\hat{{X}}_{t} \bot \mu_{t-1} \supset \mu_{s} \\ 
 u_{s} =X_{s} -\hat{{X}}_{s} \in \mu_{s} \\ 
 \end{array}} \right\} \Rightarrow u_{t} \bot u_{s} $
\item Consideramos $\left(\frac{\mu_{t} }{\sigma }, t\in Z \right)$, sistema ortonormal de $L^{2}$
\end{itemize}

Los coeficientes de Fourier de $X_{t} =\sum_{j=0}^{\infty} \lambda_{j} u_{t-j} +\upsilon_{t}, \quad t\in Z$, satisfacen:
\[
\left(\int X_{t} \frac{\mu_{t-j}}{\sigma} \right)\frac{\mu_{t-j}}{\sigma}dP=\lambda_{j} u_{t-j} 
\]
\[
\lambda_{j} =\frac{1}{\sigma^{2}}\int X_{t} u_{t-j} dP\quad j=0,1,\ldots
\]

\begin{align*}
\lambda_{o} &= \frac{1}{\sigma^{2}}\int X_{t} u_{t} dP\\
            &= \frac{1}{\sigma^{2}}\int X_{t} \left(X_{t} -\hat{X}_{t}\right)dP\\
            &= \frac{1}{\sigma^{2}}\int \left(X_{t} -\hat{X}_{t} +\hat{X}_{t} \right)\left(X_{t} -\hat{X}_{t} \right)dP\\
            &= \frac{1}{\sigma^{2}}\int \left(X_{t} -\hat{X}_{t} \right)^{2}dP+\int \hat{X}_{t} \left(X_{t} -\hat{X}_{t} \right) dP\\
            &= \frac{1}{\sigma^{2}}\sigma^{2}\\
            &=1
\end{align*}

\[\sum\lambda_{j}^{2} <\infty,\quad\text{pues}\quad \sum_{j=0}^{\infty} \lambda_{j}^{2} \leq \left|\left| X_{t} \right|\right|^{2}\]

\begin{itemize}
\item Consid\'{e}rese $\upsilon_{t} =X_{t} -\sum_{j=0}^{\infty}\lambda_{j} u_{t-j}$. Se va a verificar que $\upsilon_{t} $satisface las propiedades requeridas.
\item $\upsilon_{t} $ centrada
\item $\upsilon_{t} \in \mu_{t} $ pues $\left\{ {\begin{array}{l}
 X_{t} \in \mu_{t} \\ 
 u_{t-j} \in \mu_{t-j} \subset \mu_{t} \Rightarrow \sum\limits_{j=0}^\infty {\lambda_{j} u_{t-j} \in \mu_{t} } \\ 
 \end{array}} \right.$
\item Sea $\upsilon_{t} =\hat{{V}}_{t} +\varepsilon_{t}$, donde $\hat{V}_{t} =P_{r}^{\mu_{t-1} } \upsilon_{t} $
\end{itemize}

Se desea demostrar que $\varepsilon_{t} =0$. Se va a suponer que $\upsilon_{t} \bot u_{s}$, $s\in Z$.\newline

$\varepsilon_{t} \bot \mu_{t-1}$ y $\varepsilon_{t} \bot u_{t}$ (pues: $\upsilon_{t} \bot \mu_{t}$, $\hat{\upsilon}_{t} \in \mu_{t-1} \bot u_{t} \Rightarrow \hat{\upsilon}_{t} \bot u_{t} )$\newline

$\hat{X}_{t} \in \mu_{t-1}$ y $\varepsilon_{t} \bot \mu_{t-1} \Rightarrow \varepsilon_{t} \bot \hat{X}_{t} \Rightarrow \varepsilon_{t} \bot X_{t} =u_{t} +\hat{X}_{t}$ y $\varepsilon_{t} \bot \mu_{s}$, $s<t$.\newline

Por tanto $\varepsilon_{t} \bot \mu_{t}$; pero $\left(\varepsilon_{t} \bot \mu_{t}, \varepsilon_{t} \in \mu_{t} \right)\Rightarrow \varepsilon_{t} =0\Rightarrow \upsilon_{t} \in \mu_{t-1} $.\newline

Con el mismo razonamiento se constata que $\upsilon_{t} \in \mu_{t-2}$ y asÃ­ sucesivamente.
\[
\upsilon_{t} \in \mathop \cap\limits_{j=0}^{\infty } \mu_{t-j} 
\]

\begin{itemize}
      \item Para concluir, demostremos que $\upsilon_{t} \bot u_{s}, s\in Z$.
\end{itemize}

$u_{t}, u_{t-1},\ldots$ sistema ortogonal, que puede completarse en una base $\left(u_{s}^{'}, s\in Z \right)$ de $L^{2}\left(\Omega, A, P \right)$. Entonces $\upsilon_{t} =\sum\alpha_{s} u_{s}^{'}$.\newline

Si $s\leq t, \upsilon_{t} \bot u_{s}$ pues: $u_{s} \bot \mu_{s-1}$ y $\upsilon_{t} \in \mu_{s-1} \quad \left(v_{t} \in \mathop \cap \limits_{j=0}^{\infty } \mu_{t-j} \right)$.\newline

Si $s>t \quad \left. \begin{array}{l}
u_{s} \in \mu_{s}\quad \text{y}\quad u_{s} \bot \mu_{s-1} \\ 
\mathop \mathop \upsilon_{t} \in \mu_{t} \subset \mu_{s-1} \\ 
\end{array} \right\} \Rightarrow u_{s} \bot \upsilon_{t}$\newline

\textbf{Unicidad:}
\[
X_{t} =u_{t} +\underbrace {\left[\sum_{j=1}^{\infty} \lambda_{j} u_{t-j} +v_{t} \right]}_{\in \mu_{t-1}}
\]
Puesto que $u_{t} \bot \mu_{t-1}$, el t\'{e}rmino entre corchetes es igual a $\hat{X}_{t}$ y $u_{t} =X_{t} -\hat{X}_{t}$.



\chapter*{ANEXO B}
% \label{sec:mylabel2}
\section{ANEXO B.1: OPERADORES DE RETARDO Y AVANCE}
% \label{subsec:mylabel3}
El operador de retardo B asocia a un proceso $X=\left(X_{t}, t\in Z \right)$ el proceso $\left(Y_{t}, t\in Z \right)$ tal que $Y_{t} =BX_{t} =X_{t-1}$.\newline

Este operador es lineal; es invertible y su inverso $B^{-1}=F$se define por$FX_{t} =X_{t+1} $; F se llama operador de avance. Estos operadores satisfacen:
\[
\begin{array}{l}
 B^{n}X_{t} =X_{t-n};F^{n}X_{t} =X_{t+n} \quad \quad 
n=1,2,\mathellipsis \\ 
 \left( {\sum\limits_{i=0}^n {a_{i} B^{i}} } \right)X_{t} 
=\sum\limits_{i=0}^n {a_{i} X_{t-i} } \\ 
 \end{array}
\]
Esta \'{u}ltima igualdad describe la acci\'{o}n sobre el proceso X de un polinomio en B. Adem\'{a}s, se define $B^{0}=F^{0}=1$ (operador identidad)\newline

\textbf{SERIES EN B}\newline

De manera m\'{a}s general se pueden definir series con el operador B (o con F). Para esto, se consideran solamente procesos estacionarios.\newline

Dado un proceso estacionario $X=\left(X_{t}, t\in Z \right)$ y una sucesi\'{o}n $\left(a_{i}, i\in Z \right)$ absolutamente convergente $\sum_{i=-\infty }^\infty \left|a_{i}\right| <+\infty$, se sabe que el proceso definido por:
\[
Y_{t} =\sum\limits_{i=-\infty }^\infty a_{i} X_{t-i},\quad t\in Z
\]
es estacionario. Se denotar\'{a} por: $\sum\limits_{i=-\infty}^{\infty} a_{i} B^{i}$ a la aplicaci\'{o}n que al proceso estacionario X le hace corresponden al proceso estacionario Y.\newline

Estas series en B tienen propiedades que permiten manejarlas como a las series enteras habituales. En particular se puede sumarlas y realizar la composici\'{o}n entre ellas.\newline

\textbf{SUMAS DE SERIES EN B}

\begin{align*}
\left(\sum\limits_{i=-\infty}^{\infty} a_{i}B^{i}+\sum\limits_{i=-\infty}^{\infty} \alpha_{i} B^{i} \right)X_{t} &=\sum\limits_{i=-\infty }^{\infty} a_{i}B^{i} X_{t} +\sum\limits_{i=-\infty}^{\infty} \alpha_{i}B^{i}X_{t}\\
      &= \sum\limits_{i-\infty}^{\infty} a_{i} X_{t-i} +\sum\limits_{i=-\infty}^{\infty} \alpha_{i} X_{t-i}\\
      &= \lim\limits_{m,n\to \infty} \left(\sum\limits_{-m}^{n} a_{i}X_{t-i} +\sum\limits_{-m}^{n} \alpha_{i} X_{t-i} \right) \\ 
      &= \lim\limits_{m,n\to \infty} \left[\sum\limits_{-m}^{n} \left(a_{i} +\alpha_{i} \right)X_{t-i}\right] \\ 
      &= \sum\limits_{i=-\infty}^{\infty} \left(a_{i} +\alpha_{i} \right)X_{t-i} \\ 
      &= \left[\sum\limits_{i=-\infty}^{\infty} \left( a_{i} +\alpha_{i}\right)B^{i} \right]X_{t}
\end{align*}

pues si cada sucesi\'{o}n es absolutamente convergente la suma de las dos sucesiones tambi\'{e}n lo ser\'{a}. Fundamentalmente se observa que:
\[
\sum\limits_{i=-\infty}^{\infty} a_{i}B^{i}+\sum\limits_{i=-\infty}^{\infty} \alpha_{i}B_{i} =\sum\limits_{i=-\infty}^{\infty} \left(a_{i} +\alpha_{i} \right)B^{i}
\]
De la misma manera se puede mostrar que:
\[
\lambda \sum\limits_{i=-\infty}^{\infty} a_{i}B^{i}= \sum\limits_{i=-\infty}^{\infty} \lambda a_{i}B^{i}
\]

\textbf{COMPOSICI\'{O}N DE SERIES EN B}\newline

\textbf{Propiedad:}
\[
\left[\sum_{j=-\infty}^{\infty} \alpha_{j} B^{j} \right]\left[\sum_{i=-\infty}^{\infty} a_{i} B^{i} \right]X_{t} =\lim_{n,m,n^{'},m^{'}\to \infty } \left[\sum_{j=-m}^n \alpha_{j}B^{j} \right]\left[\sum_{i=-m^{'}}^{n^{'}} a_{i} B^{i}\right]X_{t} 
\]

\textbf{Demostraci\'{o}n:}\newline

Den\'{o}tense por: $S_{n,m}, S,\tilde{S}_{n^{'}, m^{'}} ,\tilde{S}$ las aplicaciones:
\[
\sum_{j=-m}^{n} \alpha_{j} B^{j}, \sum_{j=-\infty}^{\infty} \alpha_{j} B^{j^{'}},\sum_{i=-m^{'}}^{n^{'}} a_{i} B^{i}, \sum_{i=-\infty}^{\infty} a_{i} B^{i}
\]
Se tiene que:

\begin{align*}
\left\| S \tilde{S} X_{t} -S_{n,m} \tilde{S}_{n^{'}m^{'}} X_{t} \right\| &\leq \left\| S \tilde{S} X_{t} -S_{n,m} \tilde{S}X_{t} \right\|+\left\| S_{n,m} \tilde{S}X_{t} -S_{n,m} \tilde{S}_{n^{'}, m^{'}} X_{t}  \right\|\\
      & =\left\|S\left(\tilde{S} X_{t} \right)-S_{n,m} \left(\tilde{S}X_{t} \right)\right\|+\left\| {S_{n,m} \left(\tilde{S}X_{t} -\tilde{{S}}_{n^{'} m^{'} } X_{t}\right) \right\|\\
      & \leq \left\|S\left(\tilde{S}X_{t} \right)-S_{n,m} \left(\tilde{S}X_{t} \right) \right\|+\left(\sum\limits_{j=-m}^n \left|\alpha_{j} \right| \right)\left\|\tilde{{S}}X_{t} -\tilde{{S}}_{n^{'} m^{'} } X_{t} \right\|\\
      & \leq \left\|S\left(\tilde{S}X_{t} \right)-S_{n,m} \left(\tilde{S}X_{t}\right) \right\|+\left(\sum_{j=-\infty }^{\infty} \left| \alpha_{j} \right| \right)\left\|\tilde{S}X_{t} -\tilde{S}_{n^{'} m^{'} } X_{t} \right\|
\end{align*}

La propiedad es una consecuencia de:

\[
SY_{t} =\lim_{n,m\to \infty} S_{n,m} Y_{t}\quad \text{y}\quad \tilde{S}X_{t} =\lim_{n^{'} m^{'} \to \infty} \tilde{S}_{n^{'}, m^{'}} X_{t}
\]

(l\'{i}mites en el sentido de $L_{2}$).\newline

\textbf{Corolario:}\newline

La composici\'{o}n de dos series en B es una serie en B.\newline

\textbf{Demostraci\'{o}n:}

\begin{align*}
\left[\sum_{j=-\infty}^{\infty} \alpha_{j} B^{j} \right]\left[\sum_{i=-\infty}^{\infty} a_{i} B^{i} \right]X_{t} &= \lim_{n,m,n^{'} m^{'}\to \infty} \left[\sum_{j=-m}^n \alpha_{j}B^{j} \right]\left[\sum_{i=-m^{'}}^{n^{'}} a_{i}B^{i} \right]X_{t}\quad  \text{(l\'{i}m. en el sentido $L_{2}$)}\\
      &= \lim_{n,m,n^{'},m^{'} \to \infty} \sum_{k=-m-m^{'}}^{n+n^{'}} \left[\sum_{i=\max(-m^{'}, k-n)}^{\min (n^{'},k+m)} a_{i} \alpha_{k-i}\right]B^{k}X_{t}\\
      &= \sum_{k=-\infty}^{\infty}\left[\sum_{i=-\infty}^{\infty} a_{i} \alpha_{k-i} \right]B^{k}X_{t}
\end{align*}

puesto que $b_{k} =\displaystyle\sum_{i=-\infty}^{\infty} a_{i}\alpha_{k-i}$ existe y la serie de t\'{e}rmino general $b_{k}$ es absolutamente convergente. La sucesi\'{o}n de los $b_{k}$ es la ``convoluci\'{o}n'' de las series en $\alpha_{i}$ y $a_{j}$.\newline

\textbf{Corolario:}\newline

El producto de series en B es conmutativo:\newline

\textbf{Demostraci\'{o}n:}\newline

En efecto: $\sum\limits_{i=-\infty }^\infty {a_{i} \alpha_{k-i} } =\sum\limits_{i=-\infty }^\infty {a_{k-i} \alpha_{i} } $.\newline

\section*{ANEXO B.2: ECUACIONES EN DIFERENCIAS}
% \label{subsec:mylabel4}
Consid\'{e}rese la ecuaci\'{o}n lineal homog\'{e}nea en diferencias

\begin{equation}{
\label{ecu:01}
x_{t} -\varphi_{1} x_{t-1} -\varphi_{2} x_{t-2} -\mathellipsis -\varphi _{k} x_{t-k} =0}
\end{equation}

Se va a demostrar que su soluci\'{o}n general es:
\begin{equation}{
\label{ecu:02}
x_{t} =A_{1} G_{1}^{t} +\mathellipsis +A_{k} G_{k}^{t}}
\end{equation}

donde las $A_{1},\ldots, A_{k}$ son constantes que dependen de las condiciones iniciales y $G_{1},\ldots,G_{k}$ son las soluciones de la ecuaci\'{o}n

\begin{equation}{
\label{ecu:03}
y^{k}-\varphi_{1} y^{k-1}-\mathellipsis -\varphi_{k}=0}
\end{equation}

que se supone son distintas. Para demostrar este resultado, sustit\'{u}yase (\ref{ecu:02}) en (\ref{ecu:01}):
\[
\sum {A_{i} G_{i}^{t} -\varphi_{1} \sum {A_{i} G_{i}^{t-1} -\mathellipsis -\varphi_{k} \sum {A_{i} G_{i}^{t-k} } =\sum\limits_{i=1}^k {A_{i} G_{i}^{t-k} \left( {G_{i}^{k} -\varphi_{1} G_{i}^{k-1} -\mathellipsis -\varphi_{k} } \right)=0} } } 
\]

Con esto se ha demostrado que si $G_{i}$ satisface (\ref{ecu:03}), entonces la soluci\'{o}n (\ref{ecu:02}) satisface (\ref{ecu:01}). Obs\'{e}rvese adem\'{a}s que para que $x_{t}$ tienda a cero cuando $t\to \infty$, $G_{i}^{t}$ debe tender a cero, lo que requiere que el valor absoluto de todas las soluciones de (\ref{ecu:03}) sea menor a la unidad. 

Las expresiones anteriores pueden escribirse introduciendo el operador $B$ de retardo, definido por $(B^{0}X_{t} =1; B^{k}X_{t} =X_{t-k}, k=1,2,\ldots)$. Entonces la ecuaci\'{o}n en diferencias (\ref{ecu:01}) se expresa
\[
\left( {1-\varphi_{1} B-\mathellipsis -\varphi_{k} B^{k}} \right)x_{t} =0
\]
y se denomina \textit{ecuaci\'{o}n caracter\'{i}stica} de la ecuaci\'{o}n en diferencias a:
\begin{equation}{
\label{ecu:04}
1-\varphi_{1} y-\mathellipsis -\varphi_{k} y^{k}=0}
\end{equation}

N\'{o}tese que: $y^{k}-\varphi_{1} y^{k-1}-\mathellipsis -\varphi_{k} =0\Leftrightarrow y^{k}\left( {1-\varphi_{1} y^{-1}-\mathellipsis -\varphi_{k} y^{-k}} \right)=0$

As\'{i}, si $G_{i}$ satisface (\ref{ecu:03}) entonces $G_{i}^{-1}$ es la soluci\'{o}n de la ecuaci\'{o}n caracter\'{i}stica (\ref{ecu:04}). La soluci\'{o}n (\ref{ecu:02}) se expresa habitualmente indicando que $G_{1}^{-1},\ldots,G_{k}^{-1}$ son las ra\'{i}ces de la ecuaci\'{o}n caracter\'{i}stica.\newline

Cuando dos ra\'{i}ces de (\ref{ecu:04}) o (\ref{ecu:03}) son iguales los dos t\'{e}rminos $A_{i} G_{i}^{t} $ correspondientes pueden escribirse como $\left( {A_{1} +A_{2} t} \right) G_{i}^{t}$. Para demostrarlo, sup\'{o}ngase el caso m\'{a}s simple, con una ra\'{i}z doble en la ecuaci\'{o}n:

\begin{equation}{
\label{ecu:05}
x_{t} -\varphi_{1} x_{t-1} -\varphi_{2} x_{t-2} =0}
\end{equation}
cuya ecuaci\'{o}n caracter\'{i}stica es:

\begin{equation}{
\label{ecu:06}
\left( {1-\varphi_{1} y-\varphi_{2} y^{2}} \right)=0}
\end{equation}

Si existe una ra\'{i}z doble en esta ecuaci\'{o}n, $G_{0}^{-1}$, se verifica:
\[
\left( {1-G_{0} y} \right)^{2}=\left( {1-\varphi_{1} y-\varphi_{2} y^{2}} \right)
\]
que implica $\varphi_{2} =-G_{0}^{2}$; $\varphi_{1} =2G_{0}$ y por tanto:
\begin{equation}{
\label{ecu:07}
\varphi_{1} G_{0} +2\varphi_{2} =0}
\end{equation}

Adem\'{a}s si $G_{0}^{t} $ es una soluci\'{o}n doble de (\ref{ecu:05}) tambi\'{e}n debe serlo $tG_{0}^{t}$, pues sustituy\'{e}ndola en (\ref{ecu:05}):
\[
tG_{0}^{t} -\varphi_{1} (t-1)G_{0}^{t-1} -\varphi_{2} \left(t-2 \right)G_{0}^{t-2} =tG_{0}^{t-2} \left(G_{0}^{2} -\varphi_{1} G_{0} -\varphi_{2} \right)+G_{0}^{t-2} \left(\varphi_{1} G_{0} +2\varphi_{2} \right)=0
\]

El primer t\'{e}rmino es cero por ser $G_{0}$ soluci\'{o}n y el segundo por ser ra\'{i}z de orden 2, por esto se concluye que la soluci\'{o}n general es:
\[
x_{t} =A_{1} G_{o}^{t} +A_{2} tG_{o}^{t} =\left(A_{1} +A_{2} t \right)G_{0}^{t} 
\]

An\'{a}logamente se puede demostrar que si la ra\'{\i}z $G_{0} $ es m\'{u}ltiple, de orden $h$, la soluci\'{o}n general es:
\begin{equation}{
\label{ecu:08}
x_{t} =\left(A_{1} +A_{2} t+\mathellipsis +A_{h} t^{h-1} \right)G_{0}^{t} }
\end{equation}

Para estudiar el comportamiento de las ra\'{i}ces complejas de la ecuaci\'{o}n caracter\'{i}stica, consid\'{e}rese el caso simple determinado por las ecuaciones (\ref{ecu:05}) y (\ref{ecu:06}).\newline

Sup\'{o}ngase que $G_{1}^{-1} $ y $G_{2}^{-1} $ son soluciones distintas de la ecuaci\'{o}n caracter\'{i}stica
\[
\left(1-\varphi_{1} y-\varphi_{2} y^{2} \right)=\left(1-G_{1} y \right)\left(1-G_{2} y \right)=0
\]

Por tanto, $\varphi_{1} =G_{1} +G_{2}$; $\varphi_{2} =-G_{1} G_{2}$. Se calculan $A_{1}$ y $A_{2}$, imponiendo las condiciones iniciales $\rho_{0} =1$; $\rho_{1} =\varphi_{1} \left( {1-\varphi_{2} } \right)=\left( {G_{1} +G_{2} } \right)/\left( {1+G_{1} G_{2} } \right)$. Se obtiene:

\[
A_{1} =\frac{G_{1} \left( {1-G_{2}^{2} } \right)}{\left( {1+G_{1} G_{2} } \right)\left( {G_{1} -G_{2} } \right)};\quad A_{2} =\frac{-G_{2} \left( {1-G_{1}^{2} } \right)}{\left( {1+G_{1} G_{2} } \right)\left( {G_{1} -G_{2} } \right)}
\]

Sup\'{o}ngase adem\'{a}s que $G_{1}$ y $G_{2}$ son complejas; entonces deben ser conjugadas y pueden escribirse $G_{1} =r\exp \left(i\varpi \right)$; $G_{2} =r \exp \left(-i\varpi \right)$, donde el m\'{o}dulo se calcula por:

\begin{equation}{
\label{ecu:09}
\begin{split}
r^{2} &= G_{1} G_{2} =-\varphi_{2}\\
r&=\sqrt{-\varphi_{2}}
\end{split}
}
\end{equation}

y la frecuencia angular con:
\[
\varphi_{1} =G_{1} +G_{2} =r\left( {\exp \left( {i\varpi } \right)+\exp \left( {-i\varpi } \right)} \right)=2r\;Cos\;\varpi 
\]

Utilizando el hecho que $\varpi =2\pi /T_{0}$, donde $T_{0}$ es el per\'{i}odo, y (\ref{ecu:09}), se obtiene:
\begin{equation}{
\label{ecu:10}
Cos \varpi = Cos\frac{2\pi}{T_{0}}=\frac{\varphi_{1}}{2\sqrt{-\varphi_{2}}} }
\end{equation}

Sustituyendo $A_{1}$, $A_{2}$, $G_{1}$ y $G_{2}$ en la ecuaci\'{o}n (\ref{ecu:02}) y operando, se obtiene:
\[
x_{t} =\frac{\left[sg(\varphi_{1}) \right]r \sen\left(2\pi t/T_{0} +\alpha \right)}{tg\alpha }
\]

Donde $tg \alpha =\left(1+r^{2} \right)\;tg\left(2\pi /T_{0} \right)/\left( {1-r^{2}} \right)$ y $sg\left(\varphi_{1} 
\right)$ es el signo del coeficiente $\varphi_{1}$. Por tanto $X_{t}$ ser\'{a} una senoide amortiguada con factor de amortiguaci\'{o}n $r$ y per\'{i}odo $T_{0}$.\newline

Por tanto, la soluci\'{o}n general de (\ref{ecu:01}) tendr\'{a}: 

\begin{itemize}
\item Por cada ra\'{i}z real no repetida un t\'{e}rmino del tipo $A_{i} G_{i}^{t}$; 
\item Por cada ra\'{i}z real $G_{0}$ repetida $h$ veces un t\'{e}rmino polin\'{o}mico de orden $\left(h-1\right)$ en la variable $t$ que multiplica a $G_{0}^{t} $ (ecuaci\'{o}n \ref{ecu:08});
\item Por cada par de ra\'{i}ces complejas conjugadas un t\'{e}rmino sinusoidal.
\end{itemize}


\section{ANEXO B.3: DEMOSTRACIONES DE ALGUNOS TEOREMAS}\newline
%\label{subsec:mylabel5}

\textbf{Teorema 2.2:} Una condici\'{o}n necesaria y suficiente para que exista un proceso AR(p) que satisfaga $\pi^{'}\left( B \right)X_{t} =u_{t}$ es que las soluciones de la ecuaci\'{o}n $\pi^{'}\left( z \right)=0$ se encuentren fuera del c\'{i}rculo unidad. Bajo esta condici\'{o}n.
\[
X_{t} =\sum\limits_{j=0}^\infty {\psi_{j} u_{t-j} },\quad t\in Z
\]

donde los $\psi_{j}$, son los coeficientes de Taylor de $\frac{1}{\pi^{'}\left( z \right)}$.\newline

\textbf{Demostraci\'{o}n:}

\begin{itemize}
      \item \textbf{Condici\'{o}n Necesaria} 
\end{itemize}

Sean
\[
y_{t} =\left(\begin{array}{c}
 X_{t} \\ 
 X_{t-1} \\ 
 \vdots \\ 
 X_{t-p+1} \\ 
 \end{array}\right)
\quad
\delta_{t} =\left(\begin{array}{c}
 u_{t} \\ 
 0 \\ 
 \vdots \\ 
 0 \\ 
 \end{array}\right)
\quad
P=\left[\begin{array}{c}
 \pi_{1} \ldots \pi_{p} \\ 
 I_{p-1}\quad 0\\
 \vdots\\
 0\\
\end{array} \right]
\]

Entonces $y_{t} =Py_{t-1} +\delta_{t}$.\newline

Sean: $\Gamma=$ matriz de covarianza de $y_{t}$ y $\Delta =$ matriz de covarianza de $\delta_{t}$
\[
\Delta =
\left[ {{\begin{array}{*{20}c}
 {\sigma^{2}} \hfill & 0 \hfill & 0 \hfill \\
 0  \hfill & 0  \hfill & \hfill \\
 \vdots & & \\
 0 \hfill & \hfill & \hfill \\
\end{array} }} \right]
\]

Puesto que $\delta_{t} \bot y_{t-1}$, $\Gamma=P\Gamma P^{'}+\Delta$ (pues $\left( {X_{t} } \right)$ d.e.)

Sea $\phi \left( \lambda \right)$ el polinomio caracter\'{i}stico de P:
\[
\quad
\phi \left( \lambda \right)=\left[ {{\begin{array}{*{20}c}
 {\pi_{1} -\lambda } \hfill & {\pi_{2} } \hfill & {\pi_{3} } \hfill & 
\mathellipsis \hfill & \hfill & {\pi_{p} } \hfill \\
 1 \hfill & {-\lambda } \hfill & 0 \hfill & \hfill & \hfill & \hfill \\
 \hfill & 1 \hfill & {-\lambda } \hfill & \hfill & 0 \hfill & \hfill \\
 {\limits^{0} } \hfill & \hfill & \hfill & \hfill & 1 \hfill & {-\lambda } 
\hfill \\
\end{array} }} \right]
\]

\[
=\left( {-1} \right)^{p-1}[-1+\sum\limits_{j=1}^P {\pi_{j} \left. {\lambda^{-j}} \right]} \lambda^{P}\quad \text{(desarrollando por la primera l\'{i}nea)}.
\]


Las ra\'{i}ces $\left( {\ne 0} \right)$ de $\phi \left( \lambda \right)$son las ra\'{i}ces de $\left[ {-1+\sum\limits_{j=1}^P {\pi_{j} \lambda^{-j}} } \right]\lambda^{P}$; es decir, aquellas de $\pi^{'}\left( {\frac{1}{\lambda }} \right)=0$. Entonces los valores propios (v.p.) de \textbf{\textit{P}} son las inversas de las ra\'{i}ces de $\pi^{'}$. Se va a demostrar que estos son de $\left\| \right\|<1$.\newline

Sea $X$ un vector propio de $P^{'}$ y $\lambda $ el v.p. asociado (Recordamos que: $X^{\ast }=\overline X^{'}yX^{\ast }P=\overline \lambda X^{\ast })$; entonces

\begin{align*}
0\le X^{\ast }\Delta X &= X^{\ast }\Gamma X-X^{\ast }P\Gamma P^{'}X\\
                       &\leq \left( 1-\left| \lambda \right|^{2} \right)\underbrace {X\ast \Gamma X}_{\ge 0}\\
                       & \left| \lambda \right|^{2}\le 1
\end{align*}

Sup\'{o}ngase que existe $\lambda$ valor propio de $P^{'}$ t.q. $|\lambda |=1\Rightarrow X^{\ast}\Delta X=0$.\newline

Sea $X=\left( {\begin{array}{l}
 x_{1} \\ 
 x_{2} \\ 
 \vdots \\ 
 x_{p} \\ 
 \end{array}} \right)\Rightarrow X^{\ast }\Delta X=\sigma^{2}\left| x_{1} \right|^{2}\Rightarrow x_{1} =0$.\newline

Por tanto:
\[
P^{'}X=\left[ {{\begin{array}{*{20}c}
 {\pi_{1} } \hfill & 1 \hfill & 0 \hfill & \cdots \hfill & 0 \hfill \\
 {\pi_{2} } \hfill & 0 \hfill & 1 \hfill & \cdots \hfill & 0 \hfill \\
 \mathellipsis \hfill & \cdots \hfill & \cdots \hfill & \cdots \hfill & 
\cdots \hfill \\
 \cdots \hfill & \cdots \hfill & \cdots \hfill & \cdots \hfill & \cdots 
\hfill \\
\end{array} }} \right]\left[ {{\begin{array}{*{20}c}
 {\begin{array}{l}
 0 \\ 
 x_{2} \\ 
 x_{3} \\ 
 \vdots \\ 
 \end{array}} \hfill \\
\end{array} }} \right]=\lambda \left[ {\begin{array}{l}
 0 \\ 
 x_{2} \\ 
 \vdots \\ 
 \end{array}} \right]\Rightarrow x_{2} =0
\]
continuando con la segunda l\'{i}nea se obtiene $x_{3}=0$ etc. $\Rightarrow X=0$ (una contradicci\'{o}n, pues un vector propio no puede ser nulo).
\[
\left| \lambda \right|<1
\]

\begin{itemize}
      \item \textbf{Condici\'{o}n Suficiente}
\end{itemize}

Sea $\left( {u_{t} } \right)$ el r.b. y sea $\pi^{'}$ dado. Se busca construir el proceso AR(p).
\[
\frac{1}{\pi^{'}(z)} \quad\text{es holomorfa en}\quad |z|<1+\alpha \quad\text{($\alpha>0$, bien elegido)}
\]

Consid\'{e}rese el desarrollo de Taylor de $\frac{1}{\pi^{'}(z)}$:
\[
\frac{1}{\pi^{'}(z)}=\sum_{j=0}^{\infty} \psi_{j}z^{j},\quad \left| z \right|<1+\alpha 
\]

(la serie converge en todo c\'{i}rculo cerrado contenido en $B(0,1+\alpha)$).\newline

Para $|z|=1$, $\sum_{j=0}^{\infty} \left| \psi_{j} \right|<\infty$. Se pone: $X_{t} =\sum_{j=0}^{\infty} \psi_{j} u_{t-j}$ y luego se considera $X_{t} -\sum_{j=1}^P \pi_{j} X_{t-j} =u_{t}$.\newline

Se hacen los reemplazos correspondientes y se verifica que este proceso satisface la definici\'{o}n (c\'{a}lculos un poco largos).\newline

\textbf{Teorema 2.5:} Se tiene que $X_{t}=m_{t}+Y_{t}, \forall t\geq -s+1$, donde: 

\begin{enumerate}
      \item[a)] $m_{t}$ es la soluci\'{o}n de $\phi (B)X_{t} =0$ con $X_{-s+1},\ldots, X_{-s+p^{'}}$ como valores iniciales (v.i.)
      \item[b)] $Y_{t} =\left\{ {\begin{array}{ll}
 \displaystyle\sum_{i=0}^{t+s-p^{'}-1} \psi_{i} u_{t-i} & t\geq -s+p^{'}+1 \\ 
 0 & -s+1<t<-s+p^{'}+1 \\ 
 \end{array}} \right.$
\end{enumerate}

Los $\psi_{i}$ se obtienen por divisiones creciente de $\Theta$ por $\phi$ (hasta el orden $t+s-p^{'}-1$).\newline

\textbf{Demostraci\'{o}n: }Una soluci\'{o}n particular de:
\[
\varphi \left( B \right)X_{t} =\Theta \left( B \right)\,u_{t} \quad t\geq -s+p^{'}+1
\]
es:
\[
\left\{ {\begin{array}{ll}
 \displaystyle\sum_{i=0}^{t+s-p^{'}-1} {\psi_{i} u_{t-i} } & t\geq -s+p^{'}+1 \\ 
 0 & t<-s+p^{'}+1 \\ 
 \end{array}} \right.
\]
pues, escribiendo la identidad de la divisi\'{o}n creciente:
\[
\Theta (B)=\phi (B)\left[ {1+\psi_{1} B+.....+\psi_{t+s-p^{'}-1} B^{t+s-p^{'}-1}} \right]+B^{t+s-p^{'}}\lambda \left( B \right)
\]
y aplicando a los dos miembros $u_{t}$ se tiene:
\[
\Theta (B)u_{t} =\phi (B)\sum_{i=0}^{t+s-p^{'}-1} {\psi_{i} u_{t-i} +0} 
\]

Puesto que $m_{t}$ es soluci\'{o}n de $\phi (B)X_{t}=0$, compatible con los v.i., se obtiene el resultado anunciado. En particular:

\[
X_{t} =m_{t} +\sum_{i=0}^{t+s-p^{'}-1} {\psi_{i} u_{t-i} },\quad \forall t\geq -s+p^{'}+1
\]

\textbf{Observaci\'{o}n B.2:}

\begin{enumerate}
      \item[i.] Puesto, que $m_{t}$ satisface una ecuaci\'{o}n en diferencias y si $\varphi$ no tiene ra\'{i}ces m\'{u}ltiples, $m_{t}$ puede expresarse:
      \[
      m_{t+s} =\sum_{i=1}^d a_{i} (t+s)^{i-1}+\sum_{i=1}^p b_{i} z_{i}^{-(t+s)} 
      \]
      \[
      (t+s\geq 1); \quad z_{i} \quad \text{ra\'{i}z de}\quad  \varphi
      \]
      Si algunas ra\'{i}ces de $\varphi (z)$ son m\'{u}ltiples, hay que introducir ciertos factores polinomiales en la segunda sumatoria; pero, en todos los casos, este t\'{e}rmino se vuelve despreciable para $\forall t\geq 0$ y s suficientemente grande.\newline
      Si s es bastante grande, se puede suponer que $m_{t}$ es, para todo $t\geq 0$, un polinomio de grado d-1
      \item[ii.] $\forall r\in N$, $\Theta (z)=\varphi (z)\left[1+\displaystyle\sum_{i=1}^r \psi_{i} z^{i} \right]+z^{r+1}\lambda (z)$.\newline
      
      entonces: $\forall n\geq \max (q+1, p^{'})$, $\psi_{n} -\phi_{1} \psi_{n-1} -\ldots-\phi_{p} \psi_{n-p^{'}} =0$.\newline
      Razonando de la misma manera que en la parte (i), se puede aproximar $\psi_{n}$, para $n$ suficientemente grande, por un polinomio de grado \textit{d-1} en $n$. Por lo tanto, la serie diverge si $d>1$.
\end{enumerate}

El teorema anterior no depende del hecho que $u_{t}$ sea centrado.\newline


Si $u_{t} =\varepsilon_{t} +\lambda$ con $E\varepsilon_{t} =0$, el proceso ARIMA se presenta asÃ­: $\phi (B)X_{t} =\theta_{0} +\Theta (B)\varepsilon_{t}$ con $\theta_{0} =\lambda (1-\theta_{1} -\ldots-\theta_{q} )$ y entonces:

\[
X_{t} =m_{t} +\lambda \sum_{i=0}^{t+s-p^{'}-1} \psi_{i} + \sum_{i=0}^{t+s-p^{'}-1}\psi_{i} \varepsilon_{t-i}
\]

luego de la parte (ii), el segundo t\'{e}rmino es un polinomio de grado $d$ en $t$. Entonces la introducci\'{o}n de una constante, aumenta el grado de $E (X_{t})$ en una unidad.

\begin{itemize}
      \item \textbf{Representaci\'{o}n AR de un ARIMA:}
\end{itemize}

\textbf{Teorema 2.6: }Se tiene :
\[
X_{t} =-\sum_{i=1}^{t+s-p^{'}-1} \pi_{i} X_{t-i} + \sum_{j=s-p^{'}}^{s-1} \pi_{j,t}^{\ast } X_{-j} +u_{t} \quad \forall t>-s+p^{'}
\]
donde:

\begin{enumerate}
\item[a)] Los $\pi_{i}$ se obtienen por divisi\'{o}n creciente de $\phi$ por $\Theta (\pi_{0} =1)$.
\item[b)] Los $\pi_{s-p^{'},t}^{\ast },\ldots, \pi_{s-1,t}^{\ast } \to 0$ cuando $s\to \infty$.
\end{enumerate}

\textbf{Demostraci\'{o}n:}\newline

Se tiene que:

\begin{equation}{
\label{ecua:01}
1=\Theta (B)\left[1+\sum_{i=1}^{t+s-p^{'}-1} \theta_{i}^{\ast} B^{i} \right]+B^{t+s-p^{'}} v(B) }
\end{equation}

y aplicando $u_{t}$ a esta expresi\'{o}n se obtiene:
\begin{equation}{
\label{ecua:02}
\left[1+\sum_{i=1}^{t+s-p^{'}-1} \theta_{i}^{\ast} B^{i} \right] \Theta (B)u_{t} =u_{t} }
\end{equation}
o

\begin{equation}{
\label{ecua:03}
\left[1+\sum_{i=1}^{t+s-p^{'}-1} \theta_{i}^{\ast} B^{i} \right]\phi (B)X_{t} =u_{t} }
\end{equation}

que tiene la forma de la relaci\'{o}n enunciada (de la definici\'{o}n del ARIMA).\newline

De otro lado:
\begin{equation}{
\label{ecua:04}
\phi(B)=\Theta (B)\left[1+\sum_{i=1}^{t+s-p^{'}-1}\pi_{i}B^{i} \right]+B^{t+s-p^{'}}\mu (B) }
\end{equation}

Utilizando (\ref{ecua:01}) se obtiene:
\begin{equation}{
\label{ecua:05}
\phi(B)=\Theta (B)\left[1+\sum_{i=1}^{t+s-p^{'}-1}\theta_{i}B^{i} \right]\phi (B)+B^{t+s-p^{'}}v(B)\phi(B) }
\end{equation}

y utilizando la unicidad de la divisi\'{o}n de $\phi$ por $\Theta$, se igualan los coeficientes de $B^{i}$ ($i=1,\ldots,t+s-p^{'}-1$) en (\ref{ecua:03}) y (\ref{ecua:04}), entonces los coeficientes en (\ref{ecua:01}) son los $\pi_{i}$.\newline

Adem\'{a}s:
\begin{align*}
\pi_{s-p^{'},t}^{\ast} &= -\phi_{1}\theta_{t+s-p^{'}-1}^{\ast} -\ldots -\phi_{p'} \theta_{t+s-2p^{'}}^{\ast}\\
\pi_{s-p^{'}+1,t}^{\ast} &= -\phi_{2}\theta_{t+s-p^{'}-1}^{\ast } -\ldots -\phi_{p'} \theta_{t+s-2p^{'}+1}^{\ast}\\
\pi_{s-1,t}^{\ast } &= -\phi_{p'}\theta_{t+s-p^{'}-1}^{\ast } 
\end{align*}

Los $\pi_{s-p^{'},t}^{\ast },\ldots,\pi_{s-1,t}^{\ast} \to 0$ cuando $s\to \infty$, pues los $\theta_{i}$ son coeficientes de la divisi\'{o}n creciente de 1 por un polinomio con ra\'{i}ces de $\left| \right|>1$ y entonces tienden hacia cero cuando $i\to \infty$.\newline

\textbf{Observaci\'{o}n B.3:}

\begin{enumerate}
      \item[i.] Puesto que las ra\'{i}ces de $\Theta$ son de $\left| \right|>1$, se tiene que:
      \[
      \phi(z)=\Theta(z)\left[1+\sum_{i=1}^\infty \pi_{i} z^{i} \right]\quad \text{para}\quad |z|\leq 1
      \]

      (en realidad para $\left| z \right|< \min \left|z_{j}\right|$ con $z_{j}$ ra\'{i}z de $\Theta$).\newline

      Si $d>0$, $\phi(1)=0$ y entonces:
      \[
      0=\Theta (1)\left[ {1+\sum_{i=1}^\infty {\pi_{i} } } \right]
      \]
      y, dado que $\Theta(1)\neq 0$, se tiene:
      \[
      1+\sum_{i=1}^\infty {\pi_{i} =0} 
      \]
      \item[ii.] Si se pone:
      \[
      \phi (B)X_{t} =\theta_{0} +\Theta (B)\varepsilon_{t}\quad \text{con}\quad E\varepsilon_{t} =0
      \]
      \[
      \phi (B)X_{t} =\Theta(B) u_{t}\quad \text{con}\quad u_{t} =\varepsilon_{t} +\lambda
      \]
      y
      \[
      Eu_{t} =\lambda =\frac{\theta_{0}}{1-\theta_{1} -\ldots-\theta_{q}}
      \]
      se tiene:
      \[
      X_{t} =\lambda -\sum_{i=1}^{t+s-p'-1}\pi_{i} X_{t-i} +\sum \pi_{j,t}^{\ast } X_{-j} +\varepsilon_{t} 
      \]
      \item[iii.] La introducci\'{o}n de un mecanismo de inicializaci\'{o}n, hace que el prop\'{o}sito inicial de que el proceso ($X_{t}$) diferenciado $d$ veces sea ARMA (p,q), se cumpla solo asint\'{o}ticamente . Es decir, el proceso ARIMA
      \[
      X_{t} =m_{t} +\sum_{i=0}^{t+s-p'-1}\psi_{i} u_{t-i} \quad \psi_{0} =1, \quad t>-s+p'+1
      \]
      es t.q. $E(\Delta^{d}X_{t} -Z_{t})^{2}\to 0$, donde $Z_{t}$ es un ARMA(p, q) definido por: $\Phi (B)Z_{i} =\Theta (B)u_{t}.\newline
      
      En efecto:
      \[
      \Delta^{d}X_{t} =\Delta^{d}m_{t} +\nabla^{d}\left(\sum_{i=0}^{t+s-p'-1}\psi_{i} B^{i} \right)u_{t} 
      \]
      Puesto que:
      \[
      \Theta (B)=\varphi (B)\Delta^{d}\left(1+\sum_{i=1}^{t+s-p'-1} \psi_{i} B^{i} \right)+B^{t+s-p'}\lambda (B)
      \]
      entonces:
      \[
      \Delta^{d}\left(1+\sum_{i=1}^{t+s-p'-1} \psi_{i} B^{i}\right)=\frac{\Theta (B)}{\varphi (B)}-\frac{B^{t+s-p'}\lambda (B)}{\varphi(B)}
      \]
      \[
      \Delta^{d}X_{t} =\nabla^{d}m_{t} +\frac{\Theta (B)}{\varphi (B)}u_{t}\quad (u_{t}=0\quad  \text{si}\quad t\le -s+p')
      \]
      Pero para $t$ suficientemente grande $m_{t}$ se puede considerar un polinomio de grado $d-1$, luego $\Delta^{d}m_{t} \to 0$ si $t\to \infty$. Por tanto $Z_{t} -\frac{\Theta (B)}{\varphi (B)}u_{t}$ ($u_{t}=0$ si $t<-s+p'$) converge a cero en m.c. cuando $t\to \infty$.

\end{enumerate}


\chapter*{ANEXO C ANEXO ASOCIADO AL CAP\'{I}TULO 3}
%\label{sec:mylabel3}
\section*{ANEXO C.1: ESTIMACI\'{O}N DE M\'{A}XIMA VEROSIMILITUD NO CONDICIONAL EN UN PROCESO ARMA}
%\label{subsec:mylabel6}
\textbf{C.1.1 Caso de un MA (q)}

Consid\'{e}rese el siguiente modelo.
\[
z_{t} =\Theta (B)u_{t} =u_{t} -\sum_{j=1}^q \theta_{j} u_{t-j} \qquad t=1,2,\ldots,N
\]
Sup\'{o}ngase que los $u_{t}$ son normales (en realidad solo se requiere independencia). Este sistema se puede escribir matricialmente as\'{i}:
\[
z=M(\theta)u
\]
 con $z=\left( {{\begin{array}{*{20}c}
 {z_{1} } \hfill \\
 {z_{2} } \hfill \\
 \vdots \hfill \\
 \hfill \\
 {z_{N} } \hfill \\
\end{array} }} \right),\quad u=\left( {{\begin{array}{*{20}c}
 {u_{1-q} } \hfill \\
 {u_{2-q} } \hfill \\
 \vdots \hfill \\
 \hfill \\
 {u_{N} } \hfill \\
\end{array} }} \right)$ y M($\theta$) una matriz que depende de los $\theta_{j}$.\newline

Adem\'{a}s: $z \sim N(0,\sigma^{2}M(\theta)M(\theta)')$.\newline

Una primera idea puede ser encontrar la densidad de z para maximizar la verosimilitud; sin embargo, esto no es muy aconsejable pues se debe invertir la matriz $(M(\theta)M(\theta)')_{NxN}$. Consid\'{e}rese mas bien el sistema:

\begin{align*}
u_{1-q} &= u_{1-q}\\ 
\vdots & \qquad \vdots\\
u_{0} &= u_{0}\\
u_{1} &= z_{1}+\theta_{1}u_{0}+\ldots+\theta_{q}u_{1-q}\\
u_{2} &= z_{2}+\theta_{1}u_{1}+\ldots+\theta_{q}u_{2-q}\\
\vdots & \qquad \vdots\\
u_{N} &= z_{N}+\theta_{1}u_{N}+\ldots+\theta_{q}u_{N-q}\\
\end{align*}

Se obtiene :
\[
u = Lz + X u_{\ast} 
\]
donde las matrices $L$, $X$ y el vector $u_{\ast}$ se definen por:

\[
\begin{array}{l}
 L=\left( {\begin{array}{l}
 \;\;0 \\ 
 A_{1} (\theta ) \\ 
 \end{array}} \right)_{(N+q)xN} \quad \quad \quad \quad \quad \quad A_{1} 
(\theta )=\left( {{\begin{array}{*{20}c}
 1 \hfill & \hfill & \hfill & \hfill \\
 \cdots \hfill & 1 \hfill & 0 \hfill & \hfill \\
 \vdots \hfill & \vdots \hfill & \hfill & \hfill \\
 \cdots \hfill & \cdots \hfill & \cdots \hfill & 1 \hfill \\
\end{array} }} \right)_{NxN} \\ 
 X=\left( {\begin{array}{l}
 I_{q} \\ 
 A_{2} (\theta ) \\ 
 \end{array}} \right)_{(N+q)xq} \quad \quad \quad \quad \quad \quad u_{\ast 
} =\left( {\begin{array}{l}
 u_{1-q} \\ 
 \;\vdots \\ 
 \;\vdots \\ 
 u_{o} \\ 
 \end{array}} \right) \\ 
 \end{array}
\]
El sistema se escribe entonces:
\[
u=\left( {X\quad L} \right)\;\left( {\begin{array}{l}
 u_{\ast } \\ 
 z \\ 
 \end{array}} \right)\;,\quad \text{donde}\quad \left( {X\quad L} \right)=\left( 
{\begin{array}{l}
 I_{q\quad \quad \quad \quad } \quad 0 \\ 
 A_{2} (\theta )\quad \quad A_{1} (\theta ) \\ 
 \end{array}} \right)
\]

Se tiene que $det(XL)=1$, pues $(X L)$ es una matriz triangular inferior cuya diagonal tiene como t\'{e}rminos 1. Se obtiene as\'{i} la densidad del vector aleatorio $\left( {\begin{array}{l}
 u_{\ast } \\ 
 z \\ 
 \end{array}} \right)$, remplazando en la densidad de u (normal multivariante de orden $N+q$), el vector u por $Lz+Xu_{\ast}$:

\begin{equation}{
\label{ecua:06}
(2\pi \sigma^{2})^{(N+q)/2}\exp \left[-\frac{1}{2\sigma^{2}}(Lz+Xu_{\ast})' (Lz+Xu_{\ast }) \right] }
\end{equation}

Puesto que lo que interesa es calcular la distribuci\'{o}n marginal de z, se lo debe hacer a partir de la distribuci\'{o}n conjunta de $\left( {\begin{array}{l}
 u_{\ast } \\ 
 z \\ 
 \end{array}} \right)$.\newline

La proyecci\'{o}n ortogonal de $-Lz$ sobre el subespacio vectorial de $R^{N+q}$, generado por las columnas de $X$, est\'{a} dada por $X\hat{u}_{\ast}$, donde $\hat{u}_{\ast}=-(X'X)^{-1}X'Lz$; adem\'{a}s, el teorema de Pit\'{a}goras permite escribir:

\begin{align*}
(Lz+Xu_{\ast})'(Lz+Xu_{\ast} ) &= (-X\hat{u}_{\ast} + Xu_{\ast} )'(-X\hat{u}_{\ast} +Xu_{\ast}) + (Lz+X\hat{u}_{\ast})'(Lz+X\hat{u}_{\ast})\\
                               &= (u_{\ast} -\hat{u}_{\ast})'X'X(u_{\ast} - \hat{u}_{\ast}) + (Lz+X\hat{u}_{\ast})'(Lz+X\hat{u}_{\ast})
\end{align*}

de donde (\ref{ecua:06}) se puede expresar por:
\begin{equation}{
\label{ecua:07}
(2\pi \sigma^{2})^{-(N+q)/2}\exp \left\{-\frac{1}{2\sigma^{2}}\left[{(Lz+X\hat{u}_{\ast} )'(Lz+X\hat{u}_{\ast})+(u_{\ast} -\hat{u}_{\ast})'X'X(u_{\ast} -\hat{u}_{\ast})} \right]\right\} }
\end{equation}

Si se integra con respecto a $u_{\ast } $, se obtiene que la densidad de z es:
\[
L=(2\pi \sigma^{2})^{-N/2}\left| {X'X} \right|^{-1/2}\exp \left[-\frac{1}{2\sigma^{2}}(Lz+\hat{u}_{\ast})'(Lz+X\hat{u}_{\ast}) \right]
\]

N\'{o}tese que (\ref{ecua:07}) permite interpretar a $\hat{u}_{\ast}$ como $E(u_{\ast}| z)$.\newline

Sea $S(\theta)= (Lz+X\hat{u}_{\ast})'(Lz+X\hat{u}_{\ast})$. Consid\'{e}rese entonces:

\begin{equation}{
\label{ecua:08}
\ell =Ln\;L=-\frac{N}{2}Ln\;(2\pi )-\frac{N}{2}Ln(\sigma^{2})-\frac{1}{2}Ln\left| {X'X} \right|-\frac{S(\theta)}{2\sigma^{2}} }
\end{equation}

Para encontrar los estimadores de m\'{a}xima verosimilitud, se debe maximizar $\ell$ con respecto a $\theta$ y $\sigma^{2}$. Derivando con respecto a $\sigma^{2}$ e igualando a cero se obtiene:
\[
\frac{\partial \ell }{\partial \sigma^{2}}=-\frac{N}{2\sigma^{2}}+\frac{S(\theta )}{2\sigma^{4}}=0\Leftrightarrow \sigma^{2}=\frac{S(\theta )}{N}
\]
Reemplazando esta expresi\'{o}n de $\sigma^{2}$ en (\ref{ecua:08}) se obtiene:
\[
\overline{\ell} =-\frac{N}{2}Ln(2\pi)-\frac{N}{2}Ln\frac{S(\theta )}{N}-\frac{1}{2}Ln\left| {X'X} \right|-\frac{N}{2}
\]
Por lo cual se debe minimizar:
\[
\ell^{\ast}=N\;Ln\;S(\theta )+Ln\left| {X'X} \right|
\]

Existen formulaciones que permiten minimizar $\ell^{\ast}$, con la ayuda de m\'{e}todos num\'{e}ricos (m\'{e}todos llamados exactos). Sin embargo, se puede demostrar que cuando N es suficientemente grande, el t\'{e}rmino Ln $\left| {X'X} \right|$ se vuelve despreciable, por lo cual se puede aplicar la minimizaci\'{o}n \'{u}nicamente a $S(\theta)$; estos m\'{e}todos se dicen de m\'{i}nimos cuadrados (son aproximados).\newline

Sea $\hat{u}=E(u|z)$. Del hecho que $u=Lz+Xu_{\ast}$, se obtiene
\[
\hat{u} = Lz+X\hat{u}_{\ast} 
\]
y entonces:
\[
S(\theta) = \hat{u}'\hat{u} = \sum_{i=1-q}^N (\hat{u}_{i})^{2}
\]

Esta interpretaci\'{o}n de $S(\theta)$ permite proponer un c\'{a}lculo simple para $\theta$ dado. Se calcula inicialmente por predicci\'{o}n hacia atr\'{a}s, de la siguiente manera:

\begin{align*}
\hat{u}_{1-q} &= \hat{z}_{1-q} +\theta_{1} \hat{u}_{1-q-1} +\ldots +\theta_{q} \hat{u}_{1-2q} =z_{1-q} \quad (\hat{u}_{1-q-i} =0\quad \text{si}\quad i\geq 1)\\
\hat{u}_{2-q} &= \hat{z}_{2-q} +\theta_{1} \hat{u}_{1-q}\\
\vdots &= \qquad \vdots\\
\hat{u}_{0} &= \hat{z}_{0} +\theta_{1} \hat{u}_{-1} +\ldots +\theta_{q-1} \hat{u}_{1-q}
\end{align*}

As\'{i}, se tiene que se puede conocer $u_{\ast}$ si se conocen tambi\'{e}n los $\hat{z}_{i} =E(z_{i} | z)$ para $1=1-q,\ldots,0$. El c\'{a}lculo de los $\hat{z}_{i}$ se realiza utilizando la representaci\'{o}n en avance del proceso ($z_{t}$):
\[
z_{t} =\Theta \,(F)\varepsilon_{t} 
\]
Se requiere \'{u}nicamente utilizar las t\'{e}cnicas de predicci\'{o}n descritas anteriormente, pero tomando de manera inversa el sentido del tiempo. En particular se tiene que:

\[
\hat{z}_{0} =\sum_{i=1}^{\infty} \pi_{i} \hat{z}_{i} \approx \sum_{i=1}^N \pi_{i} \hat{z}_{i} 
\]

Los $\pi_{i}$ son los mismos que se utilizan para la predicci\'{o}n hacia delante ($\pi_{0}=$1). Luego se calcula:

\begin{align*}
\hat{z}_{-1} =-\sum_{i=1}^{\infty} \pi_{i} \hat{z}_{-1+i} \approx - \sum_{i=1}^{N+1} \pi_{i} \hat{z}_{-1+i} \\ 
\qquad\qquad \vdots \\ 
\hat{z}_{1-q} =-\sum_{i=1}^{\infty} \pi_{i} \hat{z}_{1-q+i} \approx \sum_{i=1}^{N+q+1} \pi_{i} \hat{z}_{1-q+i}
\end{align*}

Posteriormente se calculan los $\hat{u}_{i}$ de manera ascendente, con la f\'{o}rmula:

\[
\hat{u}_{i} =z_{i} +\theta_{1} \hat{u}_{i-1} +\ldots +\hat{\theta}_{q} \hat{u}_{i-q} \qquad i=1,\ldots,N
\]

Finalmente, estimados los $\hat{u}_{i}$ y los $\hat{z}_{i}$, la funci\'{o}n a minimizar solamente tiene por inc\'{o}gnitas a $\theta_{1},\ldots, \theta_{q}$, los cuales a su vez se pueden obtener con algoritmos num\'{e}ricos; sin embargo, vale la pena mencionar que para obtener el valor \'{o}ptimo de $S(\theta)$ con t\'{e}cnicas num\'{e}ricas, se requiere tener valores iniciales de buena calidad.\newline

\textbf{C.1.2 Caso de un ARMA (p , q)}\newline

Se utiliza la representaci\'{o}n MA del proceso:
\[
X_{t} =\sum_{i=0}^{\infty} \psi_{i} u_{t-i} \approx \sum_{i=0}^{Q} \psi_{i} u_{t-i} 
\]
donde Q debe ser suficientemente grande.\newline

En el caso precedente $\hat{z}_{i}=0$, para $i\leq -q$, por lo cual se puede tomar Q como el \'{i}ndice a partir del cual los $\hat{z}_{i}$ se vuelvan despreciables (por ejemplo, menores en valor absoluto a $10^{-3}$). As\'{i} entonces, para $\varphi '= (\phi_{1},\ldots,\phi_{p})$ y $\theta '= (\theta_{1},\ldots,\theta_{q})$ se debe minimizar la siguiente expresi\'{o}n 

\[
S(\varphi ,\theta )=\sum_{i=1-Q}^N (\hat{u}_{t})^{2}
\]

puesto que $\Phi(B)z_{t} =\Theta (B)u_{t}$ se puede explicitar esta relaci\'{o}n as\'{i}:
\[
z_{t} -\varphi_{1} z_{t-1} -\ldots -\varphi_{p} z_{t-p} =u_{t} -\theta_{1} \mu_{t-1} -\ldots -\theta_{q} u_{t-q} 
\]

lo que implica:

\[
\hat{u}_{t} =\hat{z}_{t} -\varphi_{1} \hat{z}_{t-1} -\ldots -\varphi_{p} \hat{z}_{t-p} +\theta_{1} \hat{u}_{t-1} +\ldots +\theta_{q} \hat{u}_{t-q} 
\]

$S(\varphi, \theta)$ se puede minimizar con diversos algoritmos num\'{e}ricos.\newline

\textbf{C.1.3 Valores iniciales}\newline

Para los valores iniciales de $\varphi_{i}$ se puede utilizar el siguiente sistema:
\[
\left( {\begin{array}{l}
 \rho_{q+1} \\ 
 \;\vdots \\ 
 \;\vdots \\ 
 \rho_{q+p} \\ 
 \end{array}} \right)=\left( {\begin{array}{l}
 \rho_{q\quad } \quad \quad \cdots \quad \rho_{q-p+1} \\ 
 \;\vdots \quad \quad \quad \;\quad \;\;\vdots \\ 
 \;\vdots \quad \quad \quad \;\quad \;\;\vdots \\ 
 \rho_{q+p-1} \quad \cdots \quad \;\rho_{q} \\ 
 \end{array}} \right)\left( {\begin{array}{l}
 \varphi_{1} \\ 
 \;\vdots \\ 
 \;\vdots \\ 
 \varphi_{p} \\ 
 \end{array}} \right)
\]
Los $\rho_{i}$ se estiman por los $\rho_{i}$, por lo cual se obtienen estimadores convergentes de los $\varphi_{i}$.\newline

Para obtener valores iniciales para los $\theta_{j}$ se utiliza la noci\'{o}n de autocorrelaciones inversas de Cleveland (1972), denotadas por $\rho_{i}(h)$. Se interpretan como las autocorrelaciones del proceso ARMA(q,p), en donde se han intercambiado los papeles de $\Phi$ y $\Theta$.\newline

Se establece un sistema lineal para $h=p+1,\ldots,p+q$, a partir de la relaci\'{o}n;
\[
\rho_{i} (h)=\sum_{j=1}^q \theta_{j} \rho_{i} (h-j) \qquad (h>p)
\]

$\rho_{i} (j)$ se reemplaza por las estimaciones (integradas en varios paquetes estad\'{i}sticos), lo que permite obtener estimaciones convergentes de los $\theta_{j}$.\newline

Si existe una constante $\theta_{0}$ en el modelo, se la puede estimar por:
\[
\hat{\theta}_{0} =\bar{z}(1-\varphi_{1} -\ldots -\hat{\varphi}_{p} )
\]

\textbf{Observaci\'{o}n C.1:}

Se puede demostrar que los estimadores obtenidos por el m\'{e}todo de m\'{a}xima verosimilitud (exacto o aproximado) son asint\'{o}ticamente normales (incluso si los $u_{i}$ son solamente independientes). La matriz de varianza -- covarianza de los estimadores se estima en el proceso iterativo.
